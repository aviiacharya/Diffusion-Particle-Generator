{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Diffusion-Based Particle Configuration Generator\n",
      "============================================================\n",
      "Using device: cpu\n",
      "\n",
      "📊 Generating training data...\n",
      "Dataset shape: torch.Size([1000, 20, 2])\n",
      "Conditions shape: torch.Size([1000])\n",
      "\n",
      "🧠 Creating diffusion model...\n",
      "\n",
      "🎯 Training diffusion model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/50 [00:05<04:30,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 11/50 [00:59<03:26,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.9515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 21/50 [01:52<02:30,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.9069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 31/50 [02:46<01:42,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.9081\n"
     ]
    }
   ],
   "source": [
    "# Project 1: Diffusion-Based Particle Configuration Generator\n",
    "# A simplified implementation demonstrating diffusion models for particle physics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class ParticleDataGenerator:\n",
    "    \"\"\"Generate synthetic particle accelerator data with collective effects\"\"\"\n",
    "    \n",
    "    def __init__(self, n_particles=50, field_strength=1.0):\n",
    "        self.n_particles = n_particles\n",
    "        self.field_strength = field_strength\n",
    "    \n",
    "    def coulomb_force(self, positions, charges):\n",
    "        \"\"\"Calculate Coulomb forces between particles\"\"\"\n",
    "        forces = np.zeros_like(positions)\n",
    "        for i in range(len(positions)):\n",
    "            for j in range(len(positions)):\n",
    "                if i != j:\n",
    "                    r_vec = positions[i] - positions[j]\n",
    "                    r_dist = np.linalg.norm(r_vec)\n",
    "                    if r_dist > 0.1:  # Avoid singularity\n",
    "                        force = charges[i] * charges[j] * r_vec / (r_dist**3)\n",
    "                        forces[i] += force\n",
    "        return forces\n",
    "    \n",
    "    def generate_configuration(self, beam_energy=1.0, charge_ratio=0.5):\n",
    "        \"\"\"Generate a particle configuration with collective effects\"\"\"\n",
    "        # Initial random positions in a beam-like distribution\n",
    "        positions = np.random.normal(0, 0.5, (self.n_particles, 2))\n",
    "        positions[:, 0] += np.random.normal(0, 0.1, self.n_particles)  # Beam direction spread\n",
    "        \n",
    "        # Assign charges (mix of positive and negative)\n",
    "        charges = np.ones(self.n_particles)\n",
    "        n_negative = int(self.n_particles * charge_ratio)\n",
    "        charges[:n_negative] = -1\n",
    "        \n",
    "        # Calculate collective forces and adjust positions\n",
    "        forces = self.coulomb_force(positions, charges)\n",
    "        \n",
    "        # Apply beam energy effect\n",
    "        energy_factor = beam_energy * 0.1\n",
    "        positions[:, 0] *= (1 + energy_factor)\n",
    "        \n",
    "        # Add collective effect displacement\n",
    "        positions += forces * 0.01\n",
    "        \n",
    "        return positions, charges, beam_energy\n",
    "\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    \"\"\"Simple diffusion model for particle configurations\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128, condition_dim=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        \n",
    "        # Network to predict noise\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim + 1, hidden_dim),  # +1 for timestep\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t, condition):\n",
    "        # Flatten positions for network input\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Concatenate with timestep and condition\n",
    "        t_embed = t.unsqueeze(1) if t.dim() == 1 else t\n",
    "        condition = condition.unsqueeze(1) if condition.dim() == 1 else condition\n",
    "        \n",
    "        input_tensor = torch.cat([x_flat, t_embed, condition], dim=1)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = self.network(input_tensor)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        return noise_pred.view(x.shape)\n",
    "\n",
    "class DiffusionTrainer:\n",
    "    \"\"\"Trainer for the diffusion model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Diffusion parameters\n",
    "        self.timesteps = 100\n",
    "        self.beta_start = 0.0001\n",
    "        self.beta_end = 0.02\n",
    "        \n",
    "        # Create beta schedule\n",
    "        self.betas = torch.linspace(self.beta_start, self.beta_end, self.timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        self.betas = self.betas.to(device)\n",
    "        self.alphas = self.alphas.to(device)\n",
    "        self.alpha_cumprod = self.alpha_cumprod.to(device)\n",
    "    \n",
    "    def add_noise(self, x, noise, t):\n",
    "        \"\"\"Add noise to data according to diffusion schedule\"\"\"\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t].view(-1, 1, 1)\n",
    "        return torch.sqrt(alpha_cumprod_t) * x + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "    \n",
    "    def train_step(self, batch_positions, batch_conditions, optimizer):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        batch_size = batch_positions.size(0)\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(batch_positions)\n",
    "        \n",
    "        # Add noise to positions\n",
    "        noisy_positions = self.add_noise(batch_positions, noise, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = self.model(noisy_positions, t.float(), batch_conditions)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = nn.MSELoss()(predicted_noise, noise)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def sample(self, n_samples, condition, shape):\n",
    "        \"\"\"Generate samples using the trained model\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Start with random noise\n",
    "            x = torch.randn(n_samples, *shape, device=self.device)\n",
    "            condition_tensor = torch.tensor([condition] * n_samples, \n",
    "                                          dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            # Reverse diffusion process\n",
    "            for t in reversed(range(self.timesteps)):\n",
    "                t_tensor = torch.tensor([t] * n_samples, dtype=torch.float32, device=self.device)\n",
    "                \n",
    "                # Predict noise\n",
    "                predicted_noise = self.model(x, t_tensor, condition_tensor)\n",
    "                \n",
    "                # Remove noise\n",
    "                alpha_t = self.alphas[t]\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "                beta_t = self.betas[t]\n",
    "                \n",
    "                x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise)\n",
    "                \n",
    "                # Add noise for non-final steps\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                    x = x + torch.sqrt(beta_t) * noise\n",
    "            \n",
    "            return x\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting Diffusion-Based Particle Configuration Generator\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Generate training data\n",
    "    print(\"\\n📊 Generating training data...\")\n",
    "    data_generator = ParticleDataGenerator(n_particles=20)  # Smaller for demo\n",
    "    \n",
    "    # Create dataset with different beam energies\n",
    "    positions_list = []\n",
    "    conditions_list = []\n",
    "    \n",
    "    for _ in range(1000):  # Generate 1000 configurations\n",
    "        beam_energy = np.random.uniform(0.5, 2.0)\n",
    "        positions, charges, energy = data_generator.generate_configuration(beam_energy=beam_energy)\n",
    "        positions_list.append(positions)\n",
    "        conditions_list.append(energy)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    positions_tensor = torch.tensor(np.array(positions_list), dtype=torch.float32)\n",
    "    conditions_tensor = torch.tensor(np.array(conditions_list), dtype=torch.float32)\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    positions_flat = positions_tensor.view(-1, positions_tensor.size(-1))\n",
    "    positions_flat_norm = scaler.fit_transform(positions_flat)\n",
    "    positions_tensor = torch.tensor(positions_flat_norm.reshape(positions_tensor.shape), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Dataset shape: {positions_tensor.shape}\")\n",
    "    print(f\"Conditions shape: {conditions_tensor.shape}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\n🧠 Creating diffusion model...\")\n",
    "    input_dim = positions_tensor.size(1) * positions_tensor.size(2)  # Flattened particle positions\n",
    "    model = SimpleDiffusionModel(input_dim=input_dim, hidden_dim=64, condition_dim=1)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = DiffusionTrainer(model, device=device)\n",
    "    \n",
    "    # Create data loader\n",
    "    dataset = TensorDataset(positions_tensor, conditions_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\n🎯 Training diffusion model...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    losses = []\n",
    "    num_epochs = 50\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        epoch_losses = []\n",
    "        for batch_positions, batch_conditions in dataloader:\n",
    "            batch_positions = batch_positions.to(device)\n",
    "            batch_conditions = batch_conditions.to(device)\n",
    "            \n",
    "            loss = trainer.train_step(batch_positions, batch_conditions, optimizer)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Generate samples\n",
    "    print(\"\\n🎲 Generating new particle configurations...\")\n",
    "    \n",
    "    # Test with different beam energies\n",
    "    test_energies = [0.8, 1.2, 1.6]\n",
    "    generated_samples = {}\n",
    "    \n",
    "    for energy in test_energies:\n",
    "        samples = trainer.sample(n_samples=5, condition=energy, shape=(20, 2))\n",
    "        # Denormalize\n",
    "        samples_flat = samples.view(-1, 2).cpu().numpy()\n",
    "        samples_denorm = scaler.inverse_transform(samples_flat)\n",
    "        samples_final = samples_denorm.reshape(samples.shape)\n",
    "        generated_samples[energy] = samples_final\n",
    "    \n",
    "    # Visualization\n",
    "    print(\"\\n📈 Creating visualizations...\")\n",
    "    \n",
    "    # Plot 1: Training loss\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Original vs Generated configurations\n",
    "    for i, energy in enumerate(test_energies):\n",
    "        plt.subplot(2, 3, i + 2)\n",
    "        \n",
    "        # Plot original sample\n",
    "        original_positions, _, _ = data_generator.generate_configuration(beam_energy=energy)\n",
    "        plt.scatter(original_positions[:, 0], original_positions[:, 1], \n",
    "                   alpha=0.6, s=30, label='Original', color='blue')\n",
    "        \n",
    "        # Plot generated sample\n",
    "        generated = generated_samples[energy][0]  # First sample\n",
    "        plt.scatter(generated[:, 0], generated[:, 1], \n",
    "                   alpha=0.6, s=30, label='Generated', color='red', marker='x')\n",
    "        \n",
    "        plt.title(f'Energy: {energy:.1f}')\n",
    "        plt.xlabel('X Position')\n",
    "        plt.ylabel('Y Position')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Plot 3: Distribution comparison\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Compare distributions for one energy level\n",
    "    energy = 1.2\n",
    "    original_positions, _, _ = data_generator.generate_configuration(beam_energy=energy)\n",
    "    generated = generated_samples[energy][0]\n",
    "    \n",
    "    plt.hist(original_positions[:, 0], alpha=0.5, bins=10, label='Original X', color='blue')\n",
    "    plt.hist(generated[:, 0], alpha=0.5, bins=10, label='Generated X', color='red')\n",
    "    plt.title('X Position Distribution')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 4: Energy conditioning effect\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    for energy in test_energies:\n",
    "        generated = generated_samples[energy][0]\n",
    "        plt.scatter(generated[:, 0], generated[:, 1], \n",
    "                   alpha=0.7, s=30, label=f'Energy {energy:.1f}')\n",
    "    \n",
    "    plt.title('Energy Conditioning Effect')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📊 Summary Statistics:\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Final training loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Generated {len(generated_samples)} different energy configurations\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'diffusion_particle_model.pth')\n",
    "    print(\"\\n💾 Model saved as 'diffusion_particle_model.pth'\")\n",
    "    \n",
    "    print(\"\\n✅ Project 1 Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, trainer, generated_samples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, trainer, samples = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47b632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
